{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a8a8bd4-cd5d-4c91-b2aa-b93655444b2b",
   "metadata": {},
   "source": [
    "# Naive Bayes in Sentiment Analysis\n",
    "This notebook going through the use of  `Naive Bayes` rules to identify either the tweet is `Positive` or `Negative`. We will cover the following steps:\n",
    "\n",
    "- Collecting dataset (`twitter_samples`).\n",
    "- Cleaning and processing datasets.\n",
    "- Train a naive bayes model on a sentiment analysis task.\n",
    "- Test Naive Bayes model\n",
    "- Predict to our own tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b971c303-f692-45d1-adf0-3a38824f4a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Automatic reloading changes from external files\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "eeb6cb85-c7b4-48ae-95b4-601ac584e1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pprint\n",
    "from utils import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3dac54-2b76-4d95-938a-366466e05424",
   "metadata": {},
   "source": [
    "### 1. Dataset Loading\n",
    "Downloading the datasets and important packages from **`nltk`** using this script\n",
    "```Python\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')\n",
    "```\n",
    "This dataset contains subsets of 5,000 positive tweets, 5,000 negative tweets, and the full set of 20,000 tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32796d2f-1400-4495-949a-7971713d3222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']\n"
     ]
    }
   ],
   "source": [
    "# Loading datasets\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "file_ids = twitter_samples.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88b67063-2c7e-4b18-b299-e026df23f7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of positive tweets: 5000\n",
      "length of negative tweets: 5000\n"
     ]
    }
   ],
   "source": [
    "# select the set of positive and negative tweets\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "print(f'''length of positive tweets: {len(positive_tweets)}\\nlength of negative tweets: {len(negative_tweets)}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ae4a144-d00b-4a93-a674-857b78017c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged '\n",
      " 'members in my community this week :)',\n",
      " '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 '\n",
      " 'and we will be able to assist you :) Many thanks!',\n",
      " '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing '\n",
      " 'track. When are you in Scotland?!',\n",
      " '@97sides CONGRATS :)',\n",
      " 'yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark '\n",
      " 'on my fb profile :) in 15 days']\n",
      "\n",
      "\n",
      "['hopeless for tmr :(',\n",
      " \"Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 \"\n",
      " 'months :(',\n",
      " '@Hegelbon That heart sliding into the waste basket. :(',\n",
      " '“@ketchBurning: I hate Japanese call him \"bani\" :( :(”\\n\\nMe too',\n",
      " 'Dang starting next week I have \"work\" :(']\n"
     ]
    }
   ],
   "source": [
    "# pretty print top five tweets\n",
    "pprint.pprint(positive_tweets[:5])\n",
    "print('\\n')\n",
    "pprint.pprint(negative_tweets[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fd820812-b4af-458a-bd1c-1b395fa4943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the datasets into train and test examples\n",
    "train_positive = positive_tweets[:4000]\n",
    "test_positive = positive_tweets[4000:]\n",
    "train_negative = negative_tweets[:4000]\n",
    "test_negative = negative_tweets[4000:]\n",
    "\n",
    "train_x = train_positive + train_negative\n",
    "test_x = test_positive + test_negative\n",
    "\n",
    "train_y = np.squeeze(np.append(np.ones((len(train_positive), 1)), np.zeros((len(train_negative), 1)), axis=0))\n",
    "test_y = np.squeeze(np.append(np.ones((len(test_positive), 1)), np.zeros((len(test_negative), 1)), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4e3ad9e3-f5b3-4255-a1a7-8d7e330f546d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_positive: 4000\n",
      "train_negative: 4000\n",
      "test_positive: 1000\n",
      "test_negative: 1000\n",
      "train_x: 8000\n",
      "train_y: 8000\n",
      "test_x: 2000\n",
      "test_y: 2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shapes = f'''\n",
    "train_positive: {len(train_positive)}\n",
    "train_negative: {len(train_negative)}\n",
    "test_positive: {len(test_positive)}\n",
    "test_negative: {len(test_negative)}\n",
    "train_x: {len(train_x)}\n",
    "train_y: {len(train_y)}\n",
    "test_x: {len(test_x)}\n",
    "test_y: {len(test_y)}\n",
    "'''\n",
    "print(shapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b811f77f-4f05-473e-ae15-75e02c1f5820",
   "metadata": {},
   "source": [
    "### 2. Data Cleaning and Processing\n",
    "In this step we are going to:\n",
    "- Lowercase\n",
    "- Remove punctuation, urls, names\n",
    "- Remove stop words\n",
    "- Stemming\n",
    "- Tokenize sentences\n",
    "\n",
    "Using helper functions (`process_tweet`) defined in `utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9e3173b9-55d8-4b2b-a7c1-21815432c946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['followfriday', 'top', 'engag']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = '#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged '\n",
    "process_tweet(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c242c2-2a39-4cc9-b3c2-e2257f0b81ef",
   "metadata": {},
   "source": [
    "Next we are going to create a frequent table between positive and negative occurance in the corpus. We will use `word_freq` function it return a dictionary witha tuple `{(word, label): freq}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "56a257c6-385a-428b-a230-35b1dbd64c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}\n"
     ]
    }
   ],
   "source": [
    "# Sample of the output\n",
    "tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n",
    "ys = [1, 0, 0, 0, 0]\n",
    "print(word_freq(tweets, ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "26a0d173-6e49-41e6-8f05-884bc06fb9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the frequence of the word\n",
    "freqs = word_freq(train_x , train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937d62db-e3cb-4646-92a1-7b2d92b33f48",
   "metadata": {},
   "source": [
    "### 3. Model Training\n",
    "We will train a naive bayes model as defined in `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "988bbcd1-4f74-4a10-8514-7e8a625a3729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprior 0.0\n",
      "loglikelihood 9085\n"
     ]
    }
   ],
   "source": [
    "# define model from model.py\n",
    "model = train_naive_bayes(freqs, train_x, train_y)\n",
    "print(\"logprior\", model[0])\n",
    "print(\"loglikelihood\", len(model[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "21045956-a812-48b7-944a-2afcc4605ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5737244858565678\n"
     ]
    }
   ],
   "source": [
    "my_tweet = 'She smiled.'\n",
    "prediction =  model_predict(model, my_tweet)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b182c08a-26f6-439f-9bf6-396c95783a3c",
   "metadata": {},
   "source": [
    "### 4. Model Testing\n",
    "Model testing to measure the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "44479ac4-a3d9-4584-9971-ef2cffa71a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.994\n"
     ]
    }
   ],
   "source": [
    "# Accuracy to the model\n",
    "accuracy = model_test(model, test_x, test_y)\n",
    "print('accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e56954d0-884b-48e7-a475-2eae6ad1eb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am happy -> 2.15\n",
      "I am bad -> -1.29\n",
      "this movie should have been great. -> 2.14\n",
      "great -> 2.14\n",
      "great great -> 4.28\n",
      "great great great -> 6.41\n",
      "great great great great -> 8.55\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to test your function\n",
    "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
    "    p = model_predict(model, tweet)\n",
    "    print(f'{tweet} -> {p:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c40ebe-c7b2-42aa-a042-ab91c51db078",
   "metadata": {},
   "source": [
    "### 5. Predict your own tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2cfcf5fd-d6c8-4659-bdb6-175d4ec47ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.80222939347889"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feel free to check the sentiment of your own tweet below\n",
    "my_tweet = 'you are bad :('\n",
    "model_predict(model, my_tweet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
